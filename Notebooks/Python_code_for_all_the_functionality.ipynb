{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Custom news Analysis"
      ],
      "metadata": {
        "id": "HSzWXp3OEm-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvx4zWeYEMcG"
      },
      "outputs": [],
      "source": [
        "pip install praw faiss-gpu textstat transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "import spacy\n",
        "from textstat import flesch_reading_ease\n",
        "import faiss\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Guardian API Key\n",
        "guardian_api_key = \"8fc95a30-a0c7-4ad9-8a62-0d8d3af818cc\"\n",
        "news_api_key = \"bfe85c82ec46409196e661c10c0957f2\"\n",
        "\n",
        "\n",
        "# Models and Tools Setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# FAISS Index Setup\n",
        "embedding_dim = 384  # Dimension of 'all-MiniLM-L6-v2' embeddings\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Global Variables to Store Articles\n",
        "article_store = []  # To store articles for retrieval\n",
        "article_embeddings = []  # To store embeddings\n",
        "\n",
        "# 1. Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text.strip()\n",
        "\n",
        "def fetch_related_content(query):\n",
        "    # Fetch from Guardian API\n",
        "    search_url_guardian = f\"https://content.guardianapis.com/search\"\n",
        "    params_guardian = {\n",
        "        \"q\": query,\n",
        "        \"api-key\": guardian_api_key,\n",
        "        \"show-fields\": \"headline,standfirst\"\n",
        "    }\n",
        "    response_guardian = requests.get(search_url_guardian, params=params_guardian)\n",
        "    articles_guardian = response_guardian.json().get('response', {}).get('results', [])\n",
        "\n",
        "    # Fetch from NewsAPI\n",
        "    search_url_newsapi = f\"https://newsapi.org/v2/everything\"\n",
        "    params_newsapi = {\n",
        "        \"q\": query,\n",
        "        \"apiKey\": news_api_key,\n",
        "        \"language\": \"en\",\n",
        "        \"sortBy\": \"relevancy\"\n",
        "    }\n",
        "    response_newsapi = requests.get(search_url_newsapi, params=params_newsapi)\n",
        "    articles_newsapi = response_newsapi.json().get('articles', [])\n",
        "\n",
        "    # Process Guardian API articles\n",
        "    for article in articles_guardian:\n",
        "        title = article.get(\"webTitle\", \"\")\n",
        "        snippet = article[\"fields\"].get(\"standfirst\", \"\")\n",
        "        if snippet:\n",
        "            combined_text = f\"{title} - {snippet}\"\n",
        "            article_store.append(combined_text)  # Store title and snippet\n",
        "            embedding = similarity_model.encode(clean_text(combined_text), convert_to_tensor=False)\n",
        "            article_embeddings.append(embedding)\n",
        "            index.add(np.array([embedding], dtype=np.float32))  # Add to FAISS index\n",
        "\n",
        "    # Process NewsAPI articles\n",
        "    for article in articles_newsapi:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        snippet = article.get(\"description\", \"\")\n",
        "        if title and snippet:\n",
        "            combined_text = f\"{title} - {snippet}\"\n",
        "            article_store.append(combined_text)  # Store title and snippet\n",
        "            embedding = similarity_model.encode(clean_text(combined_text), convert_to_tensor=False)\n",
        "            article_embeddings.append(embedding)\n",
        "            index.add(np.array([embedding], dtype=np.float32))\n",
        "def summarize_content(content):\n",
        "    content = clean_text(content)\n",
        "    input_text = \"summarize: \" + content\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 3. Summarization\n",
        "def create_knowledge_graph(summary):\n",
        "    doc = nlp(summary)\n",
        "    graph = nx.DiGraph()\n",
        "\n",
        "    # Simple relationships for common dependency types\n",
        "    dependency_map = {\n",
        "        \"nsubj\": \"subject of\",  # Subject\n",
        "        \"dobj\": \"object of\",  # Direct object\n",
        "        \"prep\": \"related to\",  # Preposition\n",
        "        \"amod\": \"describes\",  # Adjective modifier\n",
        "        \"pobj\": \"prepositional object\",  # Prepositional object\n",
        "        \"advmod\": \"modifies\",  # Adverbial modifier\n",
        "        \"ROOT\": \"main action\",  # Main verb (root of the sentence)\n",
        "        \"attr\": \"attribute of\",  # Attribute (usually a noun)\n",
        "        \"acomp\": \"complement\",  # Adjective complement\n",
        "    }\n",
        "\n",
        "    # Extract entities and relationships\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['ORG', 'GPE', 'LOC', 'PERSON', 'NORP']]  # Keep relevant entity types\n",
        "    for ent in entities:\n",
        "        if ent[1] == 'ORG' or ent[1] == 'GPE' or ent[1] == 'LOC' or ent[1] == 'PERSON':  # Check for proper nouns\n",
        "            graph.add_node(ent[0], label=ent[1])\n",
        "\n",
        "    # Store connected nodes\n",
        "    connected_nodes = set()\n",
        "\n",
        "    for token in doc:\n",
        "        # We only want to process nouns (either common nouns or proper nouns)\n",
        "        if token.pos_ in ['NOUN', 'PROPN']:\n",
        "            # Add nodes and edges for noun-related dependencies\n",
        "            if token.dep_ in dependency_map and token.head.pos_ == \"VERB\":\n",
        "                # Add an edge with a simple relationship description\n",
        "                relation = dependency_map.get(token.dep_, \"related to\")\n",
        "                graph.add_edge(token.head.text, token.text, label=relation)\n",
        "                connected_nodes.add(token.head.text)\n",
        "                connected_nodes.add(token.text)\n",
        "\n",
        "    # Remove isolated nodes (those not in connected nodes)\n",
        "    nodes_to_remove = [node for node in graph.nodes if node not in connected_nodes]\n",
        "    graph.remove_nodes_from(nodes_to_remove)\n",
        "\n",
        "    # Plot the graph with only connected noun nodes\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(graph)\n",
        "    nx.draw(graph, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=10, font_weight=\"bold\")\n",
        "    plt.title(\"Knowledge Graph - Noun Nodes Only\")\n",
        "\n",
        "    # Save the plot as PNG image\n",
        "    plt.savefig(\"knowledge_graph.png\", format=\"PNG\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Return only connected nodes (nouns)\n",
        "    return list(connected_nodes)\n",
        "\n",
        "# 5. Scores Calculation\n",
        "def sentiment_consistency(input_text, related_snippet):\n",
        "    input_sentiment = sentiment_analyzer(input_text)[0]['label']\n",
        "    related_sentiment = sentiment_analyzer(related_snippet)[0]['label']\n",
        "    return 1 if input_sentiment == related_sentiment else 0\n",
        "\n",
        "def fact_density_score(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    return len(entities) / len(text.split())\n",
        "\n",
        "def readability_score(text):\n",
        "    return flesch_reading_ease(text)\n",
        "\n",
        "def lexical_diversity_score(text):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)\n",
        "    return len(unique_words) / len(words)\n",
        "\n",
        "# 6. Aggregation\n",
        "def final_verdict(scores, weights):\n",
        "    weighted_sum = sum(score * weight for score, weight in zip(scores, weights))\n",
        "    return weighted_sum / sum(weights)\n",
        "def retrieve_similar_articles(news_text, top_k=3):\n",
        "    query_embedding = similarity_model.encode(clean_text(news_text), convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array([query_embedding], dtype=np.float32), top_k)\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        if idx < len(article_store):  # Valid index check\n",
        "            results.append(article_store[idx])\n",
        "    return results\n",
        "# 7. Main Function to Detect Fake News\n",
        "def is_fake_news(news_text):\n",
        "    if not index.is_trained or len(article_store) == 0:\n",
        "        print(\"No related content indexed. Cannot verify.\")\n",
        "        return \"Unverified\"\n",
        "\n",
        "    similar_articles = retrieve_similar_articles(news_text, top_k=2)\n",
        "    if not similar_articles:\n",
        "        print(\"No similar articles found. Cannot verify.\")\n",
        "        return \"Unverified\"\n",
        "\n",
        "    max_similarity = 0  # Track maximum similarity score\n",
        "    best_article_summary = \"\"  # Variable to store the summary of the most similar article\n",
        "    scores = []\n",
        "    sentiment_scores = []\n",
        "    fact_density_scores = []\n",
        "    readability_scores = []\n",
        "    lexical_diversity_scores = []\n",
        "\n",
        "    # Concatenate top 3 most similar articles\n",
        "    concatenated_articles = \" \".join(similar_articles)\n",
        "    concatenated_summary = summarize_content(concatenated_articles)\n",
        "\n",
        "    # Generate knowledge graph for concatenated summary\n",
        "    print(\"Generating Knowledge Graph for Concatenated Articles Summary...\")\n",
        "    create_knowledge_graph(concatenated_summary)\n",
        "\n",
        "    for article_snippet in similar_articles:\n",
        "        title, snippet = article_snippet.split(\" - \", 1)  # Extract title and snippet\n",
        "\n",
        "        summary = summarize_content(snippet)\n",
        "        similarity = util.pytorch_cos_sim(\n",
        "            similarity_model.encode(news_text, convert_to_tensor=True),\n",
        "            similarity_model.encode(summary, convert_to_tensor=True)\n",
        "        ).item()\n",
        "\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            best_article_summary = summary  # Update best article summary\n",
        "\n",
        "        sentiment_score = sentiment_consistency(news_text, snippet)\n",
        "        fact_density = fact_density_score(news_text)\n",
        "        readability = readability_score(news_text)\n",
        "        lexical_diversity = lexical_diversity_score(news_text)\n",
        "\n",
        "        # Store individual scores for averaging later\n",
        "        sentiment_scores.append(sentiment_score)\n",
        "        fact_density_scores.append(fact_density)\n",
        "        readability_scores.append(readability)\n",
        "        lexical_diversity_scores.append(lexical_diversity)\n",
        "\n",
        "        # Weighted scores\n",
        "        scores.append(final_verdict(\n",
        "            [similarity, fact_density, lexical_diversity],\n",
        "            [0.5, 0.1, 0.1]\n",
        "        ))\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    avg_fact_density = sum(fact_density_scores) / len(fact_density_scores) if fact_density_scores else 0\n",
        "    avg_readability = sum(readability_scores) / len(readability_scores) if readability_scores else 0\n",
        "    avg_lexical_diversity = sum(lexical_diversity_scores) / len(lexical_diversity_scores) if lexical_diversity_scores else 0\n",
        "\n",
        "    # Print the averages, max similarity, and final verdict\n",
        "    print(f\"Average Scores:\\n\"\n",
        "          f\"Sentiment Consistency: {avg_sentiment}\\n\"\n",
        "          f\"Fact Density: {avg_fact_density}\\n\"\n",
        "          f\"Readability: {avg_readability}\\n\"\n",
        "          f\"Lexical Diversity: {avg_lexical_diversity}\\n\")\n",
        "\n",
        "    print(f\"Maximum Similarity: {max_similarity}\")\n",
        "    print(f\"Best Article Summary: {best_article_summary}\")\n",
        "    if max_similarity > 0.4:\n",
        "        return \"Real News\"\n",
        "    elif max_similarity > 0.25:\n",
        "        return \"Likely Real News\"\n",
        "    elif max_similarity > 0.2:\n",
        "        return \"Unverified\"\n",
        "    else:\n",
        "        return \"Likely Fake News\"\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "fetch_related_content(\"Andrew garfield dates shraddha kapoor\")\n",
        "news_text = \"\"\"\n",
        "Andrew garfield dates shraddha kapoor\n",
        "\"\"\"\n",
        "\n",
        "result = is_fake_news(news_text)\n",
        "print(f\"Verdict: {result}\")\n"
      ],
      "metadata": {
        "id": "99eqno_xER01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Webscraping for xustome news"
      ],
      "metadata": {
        "id": "4DVGTC7DFbXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "def is_news_title(title):\n",
        "    news_keywords = [\n",
        "        \"breaking\", \"live\", \"update\", \"report\", \"news\",\n",
        "        \"analysis\", \"opinion\", \"interview\", \"world\",\n",
        "        \"politics\", \"economy\", \"sports\", \"entertainment\"\n",
        "    ]\n",
        "    return any(keyword in title.lower() for keyword in news_keywords)\n",
        "\n",
        "def scrape_data(url):\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        html = response.read()\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        title = soup.title.string if soup.title else \"No title found\"\n",
        "\n",
        "        paragraph = soup.find('p')\n",
        "        first_paragraph = paragraph.get_text(strip=True) if paragraph else \"No paragraph found.\"\n",
        "        return title, news_analysis, first_paragraph\n",
        "    except Exception as e:\n",
        "        return None, f\"An error occurred: {e}\", None\n",
        "\n",
        "# Input URL from the user\n",
        "url = input(\"Enter a URL to scrape: \").strip()\n",
        "title, news_analysis, first_paragraph = scrape_data(url)\n",
        "\n",
        "if title:\n",
        "    print(\"Title:\", title)\n",
        "    print(\"First Paragraph:\", first_paragraph)\n",
        "else:\n",
        "    print(news_analysis)"
      ],
      "metadata": {
        "id": "vNWIVxUVFdrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trending topics analysis\n"
      ],
      "metadata": {
        "id": "GlqHjz8FEvml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "\n",
        "# Initialize Pytrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "trending_searches = pytrends.trending_searches()\n",
        "print(trending_searches.head())\n"
      ],
      "metadata": {
        "id": "UMy19qVNEulI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Live Broadcast model"
      ],
      "metadata": {
        "id": "-EoDS-guFC7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Guardian API Key\n",
        "guardian_api_key = \"8fc95a30-a0c7-4ad9-8a62-0d8d3af818cc\"\n",
        "\n",
        "# Models and Tools Setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "\n",
        "# FAISS Index Setup\n",
        "embedding_dim = 384  # Dimension of 'all-MiniLM-L6-v2' embeddings\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Global Variables to Store Articles\n",
        "article_store = []  # To store articles for retrieval\n",
        "article_embeddings = []  # To store embeddings\n",
        "\n",
        "# Step 1: Load Pretrained Models\n",
        "model_name = \"gpt2\"  # You can use GPT2 for text generation\n",
        "summarizer_model_name = \"facebook/bart-large-cnn\"  # Pretrained model for summarization\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "summarizer_model = BartForConditionalGeneration.from_pretrained(summarizer_model_name)\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(summarizer_model_name)\n",
        "\n",
        "# Step 2: Define Preprocessing Function to Clean Conversational Text\n",
        "def clean_conversational_text(text):\n",
        "    conversational_phrases = [\n",
        "        \"I'm sorry\", \"I believe\", \"I think\", \"In my opinion\",\n",
        "        \"you know\", \"I feel\", \"I just\", \"Actually\", \"Let me tell you\"\n",
        "    ]\n",
        "\n",
        "    for phrase in conversational_phrases:\n",
        "        text = text.replace(phrase, \"\")\n",
        "\n",
        "    # Further cleanup (remove unnecessary personal pronouns like \"I\")\n",
        "    text = re.sub(r'\\bI\\b', '', text)\n",
        "    text = re.sub(r'\\bI\\'m\\b', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Step 3: Define Postprocessing Function to Ensure Formal Tone in the Output\n",
        "def postprocess_output(output):\n",
        "    conversational_phrases = [\n",
        "        \"I'm sorry\", \"I believe\", \"I think\", \"In my opinion\",\n",
        "        \"you know\", \"I feel\", \"I just\", \"Actually\", \"Let me tell you\"\n",
        "    ]\n",
        "\n",
        "    for phrase in conversational_phrases:\n",
        "        output = output.replace(phrase, \"\")\n",
        "\n",
        "    # Further cleanup to remove personal pronouns if necessary\n",
        "    output = re.sub(r'\\bI\\b', '', output)\n",
        "    output = re.sub(r'\\bI\\'m\\b', '', output)\n",
        "\n",
        "    return output.strip()\n",
        "\n",
        "# Step 4: Define Text Generation Function\n",
        "def generate_news_content(input_text):\n",
        "    # Clean input text to remove conversational phrases\n",
        "    clean_input = clean_conversational_text(input_text)\n",
        "\n",
        "    # Prepare input text for the model, without adding the prompt phrase in the final output\n",
        "    input_text = clean_input  # We only use the clean input for text generation\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Generate text using the model\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=150,\n",
        "        num_beams=5,  # Controls diversity and precision of output\n",
        "        no_repeat_ngram_size=2,  # Prevents repetitive phrases\n",
        "        temperature=0.7,  # Controls randomness in output\n",
        "        top_k=50  # Limits the number of possible words\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    news_content = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Postprocess the output to remove any conversational elements\n",
        "    return postprocess_output(news_content)\n",
        "\n",
        "# Step 5: Summarize the Whole Text First\n",
        "def summarize_text(text):\n",
        "    # Clean text before summarizing\n",
        "    clean_text = clean_conversational_text(text)\n",
        "\n",
        "    # Tokenize the text and prepare it for summarization\n",
        "    inputs = summarizer_tokenizer(clean_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Summarize the text\n",
        "    summary_ids = summarizer_model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=4,\n",
        "        max_length=150,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Step 6: Read Input from a Text File\n",
        "def read_input_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Step 7: Split Text into Chunks and Process Each Chunk\n",
        "def split_text_into_chunks(text, max_chunk_length=150):\n",
        "    # Split the text into chunks of max_chunk_length tokens\n",
        "    chunks = []\n",
        "    input_ids = tokenizer.encode(text, truncation=True, max_length=max_chunk_length)\n",
        "\n",
        "    # Split if necessary\n",
        "    for i in range(0, len(input_ids), max_chunk_length):\n",
        "        chunks.append(input_ids[i:i + max_chunk_length])\n",
        "\n",
        "    return chunks\n",
        "# 1. Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text.strip()\n",
        "\n",
        "# 2. Fetch and Store Related Content for List of Queries\n",
        "def fetch_related_content(queries):\n",
        "    for query in queries:\n",
        "        search_url = f\"https://content.guardianapis.com/search\"\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"api-key\": guardian_api_key,\n",
        "            \"show-fields\": \"headline,standfirst\"\n",
        "        }\n",
        "        response = requests.get(search_url, params=params)\n",
        "        articles = response.json().get('response', {}).get('results', [])\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get(\"webTitle\", \"\")\n",
        "            snippet = article[\"fields\"].get(\"standfirst\", \"\")\n",
        "            if snippet:\n",
        "                combined_text = f\"{title} - {snippet}\"\n",
        "                article_store.append(combined_text)  # Store title and snippet\n",
        "                embedding = similarity_model.encode(clean_text(combined_text), convert_to_tensor=False)\n",
        "                article_embeddings.append(embedding)\n",
        "                index.add(np.array([embedding], dtype=np.float32))  # Add to FAISS index\n",
        "\n",
        "# 3. Summarization\n",
        "def summarize_content(content):\n",
        "    content = clean_text(content)\n",
        "    input_text = \"summarize: \" + content\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 4. RAG Similarity Search\n",
        "def retrieve_similar_articles(news_text, top_k=3):\n",
        "    query_embedding = similarity_model.encode(clean_text(news_text), convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array([query_embedding], dtype=np.float32), top_k)\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        if idx < len(article_store):  # Valid index check\n",
        "            results.append(article_store[idx])\n",
        "    return results\n",
        "\n",
        "# 5. Aggregation\n",
        "def final_verdict(scores, weights):\n",
        "    weighted_sum = sum(score * weight for score, weight in zip(scores, weights))\n",
        "    return weighted_sum / sum(weights)\n",
        "\n",
        "# 6. Main Function to Detect Fake News\n",
        "def is_fake_news(news_text):\n",
        "    if not index.is_trained or len(article_store) == 0:\n",
        "        print(\"No related content indexed. Cannot verify.\")\n",
        "        return \"Unverified\", 0\n",
        "\n",
        "    similar_articles = retrieve_similar_articles(news_text, top_k=3)\n",
        "    if not similar_articles:\n",
        "        print(\"No similar articles found. Cannot verify.\")\n",
        "        return \"Unverified\", 0\n",
        "\n",
        "    max_similarity = 0  # Track maximum similarity score\n",
        "    for article_snippet in similar_articles:\n",
        "        title, snippet = article_snippet.split(\" - \", 1)  # Extract title and snippet\n",
        "\n",
        "        summary = summarize_content(snippet)\n",
        "        similarity = util.pytorch_cos_sim(\n",
        "            similarity_model.encode(news_text, convert_to_tensor=True),\n",
        "            similarity_model.encode(summary, convert_to_tensor=True)\n",
        "        ).item()\n",
        "        max_similarity = max(max_similarity, similarity)  # Update max similarity\n",
        "\n",
        "    print(f\"Maximum Similarity: {max_similarity}\")\n",
        "    if max_similarity > 0.4:\n",
        "        return \"Real News\", max_similarity\n",
        "    elif max_similarity > 0.25:\n",
        "        return \"Likely Real News\", max_similarity\n",
        "    elif max_similarity > 0.2:\n",
        "        return \"Unverified\", max_similarity\n",
        "    else:\n",
        "        return \"Likely Fake News\", max_similarity\n",
        "\n",
        "\n",
        "# 7. Function to Process Multiple News\n",
        "def process_news_list(news_list):\n",
        "    fetch_related_content(news_list)  # Fetch related content for all news articles\n",
        "    results = {}\n",
        "    for idx, news_text in enumerate(news_list):\n",
        "        print(f\"Processing News {idx + 1}...\")\n",
        "        verdict, max_similarity = is_fake_news(news_text)\n",
        "        results[news_list[idx]] = {\n",
        "            \"Verdict\": verdict,\n",
        "            \"Max Similarity\": f\"{max_similarity:.2f}\"  # Display similarity with 2 decimal precision\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Step 8: Example Usage\n",
        "input_file_path = \"output_audio.txt\"  # Change to the path of your input text file\n",
        "input_text = read_input_from_file(input_file_path)\n",
        "\n",
        "# Summarize the entire text first\n",
        "summary = summarize_text(input_text)\n",
        "\n",
        "# Split the summarized text into chunks (if necessary)\n",
        "chunks = split_text_into_chunks(summary, max_chunk_length=150)\n",
        "\n",
        "# Generate news-like content from each chunk\n",
        "generated_contents = []\n",
        "for chunk in chunks:\n",
        "    chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "    generated_content = generate_news_content(chunk_text)\n",
        "    generated_contents.append(generated_content)\n",
        "\n",
        "\n",
        "generated_contents1=generated_contents[0].split('\\n')\n",
        "\n",
        "\n",
        "results = process_news_list(generated_contents1)\n",
        "for news, result in results.items():\n",
        "    print(f\"{news} - Verdict: {result['Verdict']} | Max Similarity: {result['Max Similarity']}\")\n"
      ],
      "metadata": {
        "id": "WMPlpFjbFCG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evalusation over the LIAR dataset\n"
      ],
      "metadata": {
        "id": "puQb8DwBFSIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import faiss\n",
        "\n",
        "# Load a Small Chunk of the Dataset\n",
        "df_fake = pd.read_csv('/content/Fake.csv').sample(n=50, random_state=42)  # Sample 500 fake articles\n",
        "df_real = pd.read_csv('/content/True.csv').sample(n=50, random_state=42)  # Sample 500 real articles\n",
        "\n",
        "# Add Labels\n",
        "df_fake['label'] = 0  # Fake\n",
        "df_real['label'] = 1  # Real\n",
        "\n",
        "# Combine and Shuffle Data\n",
        "data = pd.concat([df_fake, df_real], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split into Train and Test\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load Embedding Model (Efficient Model)\n",
        "embed_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
        "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
        "\n",
        "# Enable GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embed_model.to(device)\n",
        "\n",
        "# Function to Encode Texts in Batches\n",
        "def encode_texts(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        inputs = embed_tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = embed_model(**inputs)\n",
        "            batch_embeddings = outputs.pooler_output.cpu().numpy()  # Pooler output for sentence embeddings\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Encode and Index Training Data\n",
        "train_embeddings = encode_texts(train_data['text'].tolist())\n",
        "index = faiss.IndexFlatL2(train_embeddings.shape[1])  # FAISS index\n",
        "index.add(train_embeddings)\n",
        "\n",
        "# Load Lightweight Generative Model\n",
        "generator_model_name = \"google/flan-t5-small\"\n",
        "generator_tokenizer = T5Tokenizer.from_pretrained(generator_model_name)\n",
        "generator_model = T5ForConditionalGeneration.from_pretrained(generator_model_name).to(device)\n",
        "\n",
        "# Retrieve Relevant Context\n",
        "def rag_retrieve(article, k=5):\n",
        "    article_embedding = encode_texts([article])\n",
        "    _, retrieved_indices = index.search(article_embedding, k)\n",
        "    return train_data.iloc[retrieved_indices[0]]['text'].tolist()\n",
        "\n",
        "# Predict Using RAG\n",
        "def rag_predict(article):\n",
        "    retrieved_texts = rag_retrieve(article, k=3)\n",
        "    input_text = f\"Classify: {article[:512]} Context: {' '.join(retrieved_texts)}\"\n",
        "    inputs = generator_tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    outputs = generator_model.generate(**inputs, max_length=50)\n",
        "    prediction = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return prediction\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(test_data):\n",
        "    y_true = test_data['label'].tolist()\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        prediction = rag_predict(row['text'])\n",
        "        if \"fake\" in prediction.lower():\n",
        "            y_pred.append(0)  # Fake\n",
        "        elif \"real\" in prediction.lower():\n",
        "            y_pred.append(1)  # Real\n",
        "        else:\n",
        "            y_pred.append(0)  # Default to Fake if unclear\n",
        "\n",
        "    # Calculate Metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "\n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "# Run Evaluation\n",
        "accuracy, f1, precision, recall = evaluate_model(test_data)\n",
        "\n",
        "# Print Metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "xejGouPZFRXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}